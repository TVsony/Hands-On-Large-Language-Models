{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KedAHg5FUNLf"
   },
   "source": [
    "**ðŸ’¡ NOTE:** We will want to use a GPU to run the examples in this notebook. In Google Colab, go to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GfrAA2LTTbus"
   },
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "# transformers: For working with pre-trained models from Hugging Face.\n",
    "# accelerate: Optimizes model training on hardware like GPUs.\n",
    "# %%capture: Suppresses the output (e.g., installation logs).\n",
    "%%capture\n",
    "!pip install transformers>=4.41.2 accelerate>=0.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7m2Md2ltUxgv"
   },
   "outputs": [],
   "source": [
    "# 2. Import Required Modules\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Purpose:\n",
    "# AutoModelForCausalLM: Loads a causal language model (designed for text generation).\n",
    "# AutoTokenizer: Loads the tokenizer corresponding to the model.\n",
    "# pipeline: Simplifies using Hugging Face models for various tasks like text generation, sentiment analysis, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ERiofRkDVlEe"
   },
   "outputs": [],
   "source": [
    "# 3. Load the Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Purpose:\n",
    "# Load the tokenizer for the Phi-3-mini-4k-instruct model from the Hugging Face repository.\n",
    "# The tokenizer converts input text into numerical tokens (IDs) for the model and converts output token IDs back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6ba7102946e04d35a4ddd5edf2372aec",
      "c7db2d4c67ef4d90a91b047a82d04ff8",
      "766d336fb76f4faab0e141f679dde01c",
      "bcd1e6272f104b568dbfbeef7940f7c5",
      "5d1efd2f60f846cc906be7677ea98b1c",
      "c3149adfc7734f1aadb3167c9f0a69ad",
      "cf737433c204447c85245e2238f2f80c",
      "6494873b0f34489794dc140995c1ef37",
      "095790d6af9249d7b35c13115a25b4a5",
      "86912fe251a04e77af1cfcb6f12d4768",
      "f2550c1f0a8848589ee2d6e9b93967a8"
     ]
    },
    "id": "fAKPHlJ-V1JM",
    "outputId": "f29ee11c-489a-4366-b5bf-09b0d0019f95"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba7102946e04d35a4ddd5edf2372aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Load the Pre-trained Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Purpose:\n",
    "# Load the pre-trained Phi-3-mini-4k-instruct model, which is optimized for causal language tasks (e.g., text generation).\n",
    "# Parameters:\n",
    "# device_map=\"cuda\": Ensures the model runs on the GPU for faster processing.\n",
    "# torch_dtype=\"auto\": Automatically chooses the appropriate precision for computations (e.g., FP16 on GPU).\n",
    "# trust_remote_code=True: Allows execution of custom model code from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nwdEQ_x6V8bK"
   },
   "outputs": [],
   "source": [
    "# 5. Create a Text-Generation Pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpLi9P1GWbNy"
   },
   "source": [
    "**Purpose:**\n",
    "Creates a pipeline for text generation, which integrates the model and tokenizer into a simple interface.\n",
    "\n",
    "**Parameters:**\n",
    "- \"text-generation\": Specifies the task type as text generation.\n",
    "- model=model: Uses the loaded Phi-3-mini-4k-instruct model.\n",
    "- tokenizer=tokenizer: Uses the loaded tokenizer for tokenizing input and decoding output.\n",
    "- return_full_text=False: Ensures the output only includes the generated text, not the input prompt.\n",
    "- max_new_tokens=50: Limits the generated text to 50 tokens.\n",
    "- do_sample=False: Disables randomness, making the output deterministic (greedy decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKkoWyfXW0RF",
    "outputId": "f3ce9bd2-4490-47a6-d0ca-5a31a314a5ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' AI, or Artificial Intelligence, refers to the creation of machines or software that can perform tasks that typically require human intelligence. These tasks include learning, problem-solving, recognizing patterns, understanding language, and making decisions.'}]\n"
     ]
    }
   ],
   "source": [
    "# 6. Pipeline Functionality\n",
    "# Input: You can pass text prompts directly to the generator object. For example:\n",
    "\n",
    "output = generator(\"Explain the concept of AI in simple terms.\")\n",
    "print(output)\n",
    "\n",
    "# Output: The model will generate a continuation of the provided prompt, up to max_new_tokens tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0l_TH6GXO2n"
   },
   "source": [
    "**Benefits of Using pipeline**\n",
    "- Simplified API: Abstracts away tokenization and model inference complexities.\n",
    "- Flexibility: Supports various tasks beyond text generation, such as sentiment analysis, translation, etc.\n",
    "- Customizability: Allows fine-tuning with parameters like max_new_tokens and do_sample.\n",
    "\n",
    "This setup is now ready to generate text efficiently with the Phi-3-mini-4k-instruct model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgO8L2y_XFgG",
    "outputId": "9717a2f2-2fc0-4eac-acac-cc5d3373e6c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mention the steps you're taking to prevent it in the future.\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in\n"
     ]
    }
   ],
   "source": [
    "# 7. Set a Prompt and Generate Text\n",
    "\n",
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "\n",
    "output = generator(prompt)\n",
    "\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "# Purpose: Generates text based on the prompt using the previously created generator pipeline.\n",
    "# Output: The generated text is printed, showing the model's response to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sHJWOVIJYXQT",
    "outputId": "058fc218-a85f-4f42-e915-3f0f98e3dbfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi3ForCausalLM(\n",
      "  (model): Phi3Model(\n",
      "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Phi3DecoderLayer(\n",
      "        (self_attn): Phi3Attention(\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "          (rotary_emb): Phi3RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Phi3MLP(\n",
      "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (activation_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Phi3RMSNorm()\n",
      "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_attention_layernorm): Phi3RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Phi3RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Bvoklhm5YhFF"
   },
   "outputs": [],
   "source": [
    "# Tokenization of a Prompt\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "# Purpose: Converts the prompt into token IDs and moves them to the GPU for processing.\n",
    "# return_tensors=\"pt\": Ensures the output is in PyTorch tensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_M2rmObTY_wV"
   },
   "outputs": [],
   "source": [
    "# Accessing Model Outputs\n",
    "# Get the output of the model before the lm_head\n",
    "model_output = model.model(input_ids)\n",
    "\n",
    "# Get the output of the lm_head\n",
    "lm_head_output = model.lm_head(model_output[0])\n",
    "\n",
    "# Purpose:\n",
    "# model.model(input_ids): Extracts the raw outputs of the Transformer model before applying the language model head (lm_head).\n",
    "# model.lm_head(model_output[0]): Applies the final layer (lm_head) to get the logits for each token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7yNb6ZFqZARI",
    "outputId": "8743db6e-334e-40d0-e531-73468e52a678"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token Decoding\n",
    "\n",
    "token_id = lm_head_output[0, -1].argmax(-1)\n",
    "tokenizer.decode(token_id)\n",
    "\n",
    "# Purpose:\n",
    "#lm_head_output[0, -1]: Focuses on the logits for the last token in the sequence.\n",
    "# .argmax(-1): Identifies the token ID with the highest probability.\n",
    "# tokenizer.decode(token_id): Converts the predicted token ID back into a human-readable word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqU_KGtfZZUz",
    "outputId": "dbf65791-2553-4b95-a128-d1be2b06b3eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3072])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "soLVIZaPZg6X",
    "outputId": "5d51722d-8bf3-41a6-92f9-70bd992703a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32064])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rv8WK0gxZkkH"
   },
   "outputs": [],
   "source": [
    "# Generating a Longer Text\n",
    "\n",
    "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "# Purpose: Prepares the input tokens for generating a longer response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y33QL_SRZ6mH",
    "outputId": "96f8bb2a-d657-473d-ac55-4eb8f61c1576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time with Cache: 7.5783 seconds\n",
      "Execution Time without Cache: 28.1427 seconds\n"
     ]
    }
   ],
   "source": [
    "# Timing Text Generation with %%timeit\n",
    "import time\n",
    "\n",
    "# Timing with `use_cache=True`\n",
    "start_time = time.time()\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=True\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time with Cache: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# Timing with `use_cache=False`\n",
    "start_time = time.time()\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=False\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time without Cache: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TUbsTtpaNC2"
   },
   "source": [
    "**Purpose:**\n",
    "- %%timeit: Measures the execution time of the text generation process.\n",
    "- use_cache=True:\n",
    "   - Enables caching of key-value pairs for faster inference in\n",
    "  autoregressive decoding.\n",
    "   - Suitable for long text generation tasks.\n",
    "- use_cache=False:\n",
    "  - Disables caching, potentially slowing down inference but using less memory.\n",
    "- Comparison: Observes the performance difference when caching is enabled vs. disabled.\n",
    "\n",
    "#### Summary of the Code\n",
    "1. Generates text using both a pipeline and direct model calls.\n",
    "2. Demonstrates tokenization, model output inspection, and decoding.\n",
    "3. Explores the performance of text generation with and without caching.\n",
    "4. Provides insight into internal operations of a Hugging Face model, such as logits extraction and token prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QE4B31QZaA6S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "095790d6af9249d7b35c13115a25b4a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d1efd2f60f846cc906be7677ea98b1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6494873b0f34489794dc140995c1ef37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ba7102946e04d35a4ddd5edf2372aec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c7db2d4c67ef4d90a91b047a82d04ff8",
       "IPY_MODEL_766d336fb76f4faab0e141f679dde01c",
       "IPY_MODEL_bcd1e6272f104b568dbfbeef7940f7c5"
      ],
      "layout": "IPY_MODEL_5d1efd2f60f846cc906be7677ea98b1c"
     }
    },
    "766d336fb76f4faab0e141f679dde01c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6494873b0f34489794dc140995c1ef37",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_095790d6af9249d7b35c13115a25b4a5",
      "value": 2
     }
    },
    "86912fe251a04e77af1cfcb6f12d4768": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcd1e6272f104b568dbfbeef7940f7c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86912fe251a04e77af1cfcb6f12d4768",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f2550c1f0a8848589ee2d6e9b93967a8",
      "value": "â€‡2/2â€‡[00:37&lt;00:00,â€‡17.72s/it]"
     }
    },
    "c3149adfc7734f1aadb3167c9f0a69ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7db2d4c67ef4d90a91b047a82d04ff8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3149adfc7734f1aadb3167c9f0a69ad",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cf737433c204447c85245e2238f2f80c",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "cf737433c204447c85245e2238f2f80c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f2550c1f0a8848589ee2d6e9b93967a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
